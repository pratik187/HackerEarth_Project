{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "Several Columns which are redundant in the data.\n",
    "\n",
    "below is a list of columns which can be representative of the other mentioned columns along with it. \n",
    "\n",
    "date_of_birth : dob_year, dob_month\n",
    "age_upon_intake(days): age_upon_intake,age_upon_intake(years),age_upon_intake_age_group\n",
    "intake_datetime: converted to unixtime_intake_datetime , [intake_month\tintake_year\tintake_monthyear intake_hour] are redundant\n",
    "time_in_shelter_days: time_in_shelter, age_upon_outcome(combnation of both age_upon_intake and time_in_shelter)\n",
    "[age_upon_outcome_(days)\tage_upon_outcome_(years)\tage_upon_outcome_age_group\toutcome_datetime\toutcome_month\toutcome_year\toutcome_monthyear outcome_hour]\n",
    "these are also redundant.\n",
    "outcome_number same as intake_number\n",
    "\n",
    "count is constant\n",
    "removing all the unnecessary columns 14 columns remain except the target variable : outcome_type\n",
    "\n",
    "training and test sets have different data.\n",
    "breed in test set has 308 breed names which are not there at training set.\n",
    "for color it is 86.\n",
    "\n",
    "Hence removing these two variables leads to 12 variables.\n",
    "animal_id_outcome was primarily considered as a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47803\n",
      "47803\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "l = df['animal_id_outcome'].tolist()\n",
    "print(len(l))\n",
    "print(len(df))\n",
    "\n",
    "a1 = []\n",
    "a2 = []\n",
    "a3 = []\n",
    "a4 = []\n",
    "a5 = []\n",
    "a6 = []\n",
    "a7 = []\n",
    "count = 0 \n",
    "for animal_id in l:\n",
    "    a1.append(animal_id[0])\n",
    "    a2.append(animal_id[1])\n",
    "    a3.append(animal_id[2])\n",
    "    a4.append(animal_id[3])\n",
    "    a5.append(animal_id[4])\n",
    "    a6.append(animal_id[5])\n",
    "    a7.append(animal_id[6])\n",
    "\n",
    "df.insert(1,\"animal_id_1\", a1)\n",
    "arr = (a1,a2,a3,a4,a5,a6,a7)\n",
    "for i in range(2,8):\n",
    "    df.insert(i,\"animal_id_{}\".format(i),arr[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But these variables didn't have much impact on the model performance hence removed.\n",
    "\n",
    "best performing model is adaboost and random forest ensembled model with accuracy score of 63% in test set.\n",
    "source codes have been attached below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from time import strptime\n",
    "from datetime import datetime\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "def convertunix(a):\n",
    "    mytm= (a)\n",
    "    fmt = (\"%m/%d/%Y %H:%M\")\n",
    "    epochDate = int(time.mktime(time.strptime(mytm, fmt)))\n",
    "    return epochDate\n",
    "\n",
    "df['unixtime_birth_day'] = df.apply(lambda row: convertunix(row.date_of_birth), axis = 1)\n",
    "df['unixtime_intake_datetime'] = df.apply(lambda row: convertunix(row.intake_datetime), axis = 1)\n",
    "\n",
    "\n",
    "df = df.drop([\"count\",\"dob_year\",\n",
    "              \"dob_month\",\"age_upon_intake\",\n",
    "              \"age_upon_intake_(years)\",\"age_upon_intake_age_group\",\n",
    "              \"intake_datetime\",\"intake_month\",\"intake_year\",\n",
    "              \"intake_monthyear\",\"intake_hour\",\"time_in_shelter\",\n",
    "              \"age_upon_outcome\",\"date_of_birth\",\"age_upon_outcome_(days)\",\n",
    "              \"age_upon_outcome_(years)\",\"age_upon_outcome_age_group\",\n",
    "              \"outcome_datetime\",\"outcome_month\",\"outcome_year\",\n",
    "              \"outcome_monthyear\",\"outcome_hour\",\"outcome_number\",\"animal_id_outcome\"], axis = 1)\n",
    "df = df.dropna()\n",
    "# converting categorical to numeric variables.\n",
    "\n",
    "def category_dict_creation(column_name):\n",
    "    dictionary = {}\n",
    "    categorial_outcome = df[column_name].unique().tolist()\n",
    "    for i in range(len(categorial_outcome)):\n",
    "        dictionary[categorial_outcome[i]] = i\n",
    "    return dictionary\n",
    "\n",
    "def map_values_to_list(df1,dictionary,column_name):\n",
    "    dfl = df1.copy()\n",
    "    categorial_outcome = df1[column_name].unique().tolist()\n",
    "    outcome_list = []\n",
    "    for i in dfl[column_name]:\n",
    "        outcome_list.append(dictionary[i])\n",
    "    dfl.insert(1,column_name+\"_int\",outcome_list)\n",
    "    dfl = dfl.drop(column_name , axis = 1)\n",
    "    return dfl\n",
    "\n",
    "def convert_category_int(df,dictionary):\n",
    "    dfl = df.copy()\n",
    "    dtypes = dfl.dtypes\n",
    "    for colname in dfl.columns:\n",
    "        if dtypes[colname] == \"object\" :\n",
    "#             dictionary = category_dict_creation(colname)\n",
    "            dfl = map_values_to_list(dfl,dictionary[colname],colname)\n",
    "    return dfl\n",
    "\n",
    "dtypes = df_target.dtypes\n",
    "dict_of_dicts = {}\n",
    "for colname in df_target.columns:\n",
    "    if dtypes[colname] == \"object\":\n",
    "        dictionary = category_dict_creation(colname)\n",
    "        dict_of_dicts[colname] = dictionary\n",
    "\n",
    "df1 = convert_category_int(df_target,dict_of_dicts)\n",
    "with open('categorical_variable_mapping.txt', 'w') as outfile:\n",
    "    json.dump(dict_of_dicts, outfile)\n",
    "\n",
    "target = pd.DataFrame(df[\"outcome_type\"])\n",
    "target_int = pd.DataFrame(df1[\"outcome_type_int\"])\n",
    "X = df1.drop([\"outcome_type_int\",\"color_int\",\"breed_int\"], axis = 1)\n",
    "\n",
    "target.to_csv(\"target.csv\")\n",
    "target_int.to_csv(\"target_int.csv\")\n",
    "X.to_csv(\"processed_final_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost naive_bayes and logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "X = pd.read_csv(\"processed_final_data.csv\")\n",
    "y = pd.read_csv(\"target_int.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=467)\n",
    "D_train = xgb.DMatrix(X_train, label=y_train)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)\n",
    "xg = xgb()\n",
    "pprint(xg.get_params())\n",
    "\n",
    "param = {\n",
    "    \"eta\" : 0.25 ,\n",
    "     \"max_depth\" : 10 ,\n",
    "#      \"min_child_weight\" : 3,\n",
    "#      \"gamma\"            : 0.2 ,\n",
    "#      \"colsample_bytree\" : 0.4,\n",
    "    'num_class': 9} \n",
    "\n",
    "steps = 100  # The number of training iterations\n",
    "model = xg.train(param, D_train, steps)\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds, average='micro')))\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds, average='micro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below modelling has been done using naive bayes and Logistic but SVC gave better result than them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "X = pd.read_csv(\"processed_final_data.csv\")\n",
    "\n",
    "y = pd.read_csv(\"target.csv\")\n",
    "\n",
    "print(\"##### Data imported #########\")\n",
    "X_norm = (X-X.min())/(X.max()-X.min())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.05, random_state=467)\n",
    "\n",
    "svc=SVC(probability=True, kernel='linear')\n",
    "\n",
    "# Create adaboost classifer object\n",
    "abc =AdaBoostClassifier(n_estimators=50,base_estimator=svc, learning_rate=1)\n",
    "\n",
    "print(\"####### model training started ###########\")\n",
    "# Train Adaboost Classifer\n",
    "model = abc.fit(X_train, y_train)\n",
    "\n",
    "# prediction\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",accuracy_score(y_test, y_pred))\n",
    "\n",
    "filename = 'support_vector.sav'\n",
    "joblib.dump(model, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest with adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best set of parameter finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = pd.read_csv(\"processed_final_data.csv\")\n",
    "target = pd.read_csv(\"target.csv\")\n",
    "print(\"data import done  #############\")\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.05, random_state=467)\n",
    "#simple modellling\n",
    "#Random Forest classifier\n",
    "rf = RFC( random_state = 34)\n",
    "#classifier.fit(X_train,y_train)\n",
    "# Number of trees in random forest\n",
    "n_estimators = [50]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,'max_features': max_features,\n",
    "               'max_depth': max_depth,'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,'bootstrap': bootstrap}\n",
    "# pprint(random_grid)\n",
    "# Random search of parameters, using 3 fold cross validation,\n",
    "# search across 100 different combinations, and use all available cores\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # Fit the random search model\n",
    "        rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "        rf_random.fit(X , target)\n",
    "        print(rf_random.best_params_)\n",
    "    except Exception as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modelling with the optimum parameters obtained from the above section for RandomForest with adaboost: This modelling gave best result among all with 63% accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "X = pd.read_csv(\"processed_final_data.csv\")\n",
    "X = X.drop([])\n",
    "y = pd.read_csv(\"target.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=467)\n",
    "\n",
    "random_forest_classifier = RFC(n_estimators = 50,min_samples_split = 2, min_samples_leaf = 4, max_features = 'sqrt', max_depth = 110, bootstrap = True, random_state = 34, n_jobs=1)\n",
    "abc =AdaBoostClassifier(n_estimators= 100 ,base_estimator=random_forest_classifier, learning_rate=1)\n",
    "\n",
    "model = abc.fit(X, y)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\",accuracy_score(y_test, y_pred))\n",
    "\n",
    "filename = 'adaboost_model.sav'\n",
    "joblib.dump(model, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv1D and dense layers used : final model gave 62% accuracy on test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.externals import joblib\n",
    "# from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from time import time\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "X = pd.read_csv(\"processed_final_data.csv\")\n",
    "X = X.drop([\"breed_int\",\"color_int\"], axis =1)\n",
    "y = pd.read_csv(\"target_int.csv\")\n",
    "print(\"######## data imported ########\")\n",
    "X_norm = (X-X.min())/(X.max()-X.min())\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.asarray(X_norm), np.asarray(y), test_size=0.05, random_state=467)\n",
    "\n",
    "# The known number of output classes.\n",
    "num_classes = 9\n",
    "\n",
    "# Input image dimensions\n",
    "input_shape = (12,)\n",
    "\n",
    "# Convert class vectors to binary class matrices. This uses 1 hot encoding.\n",
    "y_train_binary = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_binary = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "X_train = X_train.reshape(45411,12,1)\n",
    "X_test = X_test.reshape(2391,12,1)\n",
    "\n",
    "print(\"######## Defining the neuralnet model ###########\")\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model = Sequential()\n",
    "model.add(Conv1D(160, (3), input_shape=(12,1), activation='relu'))\n",
    "model.add(Conv1D(160, (3), activation='relu'))\n",
    "model.add(Conv1D(160, (3), activation='relu'))\n",
    "model.add(Conv1D(160, (3), activation='relu'))\n",
    "\n",
    "\n",
    "# model.add(Conv1D(160, (3), activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(MaxPooling1D(2))\n",
    "# model.add(Conv1D(160, (10), activation='relu'))\n",
    "# model.add(Conv1D(160, (10), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(64, activation='softmax'))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "tensorboard = TensorBoard(log_dir = \"log/{}\".format(time()))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 200\n",
    "epochs = 10\n",
    "print(\"########### model training ##############\")\n",
    "model = model.fit(X_train, y_train_binary,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test_binary), callbacks=[tensorboard])\n",
    "model.evaluate(X_test, Y_test)\n",
    "\n",
    "filename = 'neural_net_model.sav'\n",
    "joblib.dump(model, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding output on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "\n",
    "#load processed test data\n",
    "df = pd.read_csv(\"modified_test_data.csv\")\n",
    "loaded_model = joblib.load(\"adaboost_model.sav\")\n",
    "X = df.drop([\"animal_id_outcome\"], axis =1)\n",
    "y_pred = loaded_model.predict(X)\n",
    "array = df[\"animal_id_outcome\"].tolist()\n",
    "result = pd.DataFrame({\"animal_id_outcome\":array,\"outcome_type\":y_pred})\n",
    "\n",
    "result.to_csv(\"result.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
